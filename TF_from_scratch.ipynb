{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6b84b3e1-dd99-4ac9-ac2a-5b7a12509cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4d418b90-3dea-4b2c-8480-51eaa49d11dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e325117b-ec06-4056-9407-dcc8343dc95c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f849f377-9792-4b5c-9559-63d9ef063a6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder:\n",
    "    def __init__(self, vocab_size, seq_len, emb_dim, linear_layers_hidden_dim, ffn_hiiden_dim):\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.linear_layers_hidden_dim = linear_layers_hidden_dim\n",
    "        self.ffn_hiiden_dim = ffn_hiiden_dim\n",
    "\n",
    " \n",
    "    def get_sin_output(self, pos):\n",
    "        t = tf.multiply(tf.constant(2, dtype=tf.float32), tf.range(0, self.emb_dim, dtype=tf.float32))\n",
    "        t1 = tf.pow(tf.constant(10000, dtype=tf.float32), tf.divide(t, self.emb_dim))\n",
    "        position_tensor = tf.fill([1, self.emb_dim], pos)\n",
    "        sin_fun_input = tf.divide(position_tensor, t1)\n",
    "        output = tf.sin(sin_fun_input)\n",
    "        return output\n",
    "    \n",
    "    def get_cos_output(self, pos):\n",
    "        t = tf.multiply(tf.constant(2, dtype=tf.float32), tf.range(0, self.emb_dim, dtype=tf.float32))\n",
    "        t1 = tf.pow(tf.constant(10000, dtype=tf.float32), tf.divide(t, self.emb_dim))\n",
    "        position_tensor = tf.fill([1, self.emb_dim], pos)\n",
    "        sin_fun_input = tf.divide(position_tensor, t1)\n",
    "        output = tf.cos(sin_fun_input)\n",
    "        return output\n",
    "    \n",
    "    def call(self, X):\n",
    "\n",
    "        with tf.compat.v1.Session() as attention_head:\n",
    "            \n",
    "            # X is data such as [[hello, world],[amazing, code]]. It is in the format of int32       \n",
    "            # first embedding layer\n",
    "            # start of embedding graph\n",
    "            inputs = tf.compat.v1.placeholder(dtype=tf.float32, shape=(1, self.vocab_size), name='InputPlaceholder')\n",
    "            input_pos = tf.compat.v1.placeholder_with_default(0.0, shape=(), name='input_pos')\n",
    "            w1 = tf.Variable(tf.random.normal(shape=(self.vocab_size, self.emb_dim), dtype=tf.float32), name='w1')\n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=(1, self.emb_dim), dtype=tf.float32), name='b1')\n",
    "            y1 = tf.add(tf.matmul(inputs, w1), b1)\n",
    "            # end of embedding graph\n",
    "            \n",
    "            #positional embeddings \n",
    "            @tf.function\n",
    "            def conditoinal(pos):\n",
    "                if tf.equal(tf.experimental.numpy.mod(pos,2) ,0):\n",
    "                    return self.get_sin_output(pos)\n",
    "                return self.get_cos_output(pos)\n",
    "            \n",
    "            positional_out = tf.add(conditoinal(input_pos), y1)\n",
    "            # end of positional output\n",
    "            \n",
    "            \n",
    "            #value linear layer\n",
    "            inputs_v = tf.compat.v1.placeholder(dtype=tf.float32, shape=(self.seq_len, self.emb_dim), name='linear_layer_graph_placeholder_v')\n",
    "            w1_v = tf.Variable(tf.random.normal(shape=(self.emb_dim, self.linear_layers_hidden_dim), dtype=tf.float32), name='w1_v')\n",
    "            b1_v = tf.Variable(tf.constant(0.1, shape=(1, self.linear_layers_hidden_dim), dtype=tf.float32), name='b1_v')\n",
    "            y1_v = tf.add(tf.matmul(inputs_v, w1_v), b1_v)\n",
    "            \n",
    "            #key linear layer\n",
    "            inputs_k = tf.compat.v1.placeholder(dtype=tf.float32, shape=(self.seq_len, self.emb_dim), name='linear_layer_graph_placeholder_k')\n",
    "            w1_k = tf.Variable(tf.random.normal(shape=(self.emb_dim, self.linear_layers_hidden_dim), dtype=tf.float32), name='w1_k')\n",
    "            b1_k = tf.Variable(tf.constant(0.1, shape=(1, self.linear_layers_hidden_dim), dtype=tf.float32), name='b1_k')\n",
    "            y1_k = tf.add(tf.matmul(inputs_k, w1_k), b1_k)\n",
    "            \n",
    "            #query linear layer\n",
    "            inputs_q = tf.compat.v1.placeholder(dtype=tf.float32, shape=(self.seq_len, self.emb_dim), name='linear_layer_graph_placeholder_q')\n",
    "            w1_q = tf.Variable(tf.random.normal(shape=(self.emb_dim, self.linear_layers_hidden_dim), dtype=tf.float32), name='w1_q')\n",
    "            b1_q = tf.Variable(tf.constant(0.1, shape=(1, self.linear_layers_hidden_dim), dtype=tf.float32), name='b1_q')\n",
    "            y1_q = tf.add(tf.matmul(inputs_q, w1_q), b1_q)\n",
    "            \n",
    "            \n",
    "            transposed_key_mat = tf.transpose(y1_k, name='key_transpose_op')\n",
    "            attention_filter = tf.divide(tf.matmul(y1_q, transposed_key_mat), tf.sqrt(tf.constant(self.emb_dim, dtype=tf.float32)), name='attention_filter')\n",
    "            softmaxed_attention_filter = tf.nn.softmax(attention_filter, name='softmax_of_attention_filter')\n",
    "            final_output_of_one_head = tf.matmul(softmaxed_attention_filter, y1_v, name='final_matmul')\n",
    "            \n",
    "            # final linear layer\n",
    "            inputs_final = tf.compat.v1.placeholder(dtype=tf.float32, shape=(self.seq_len, self.linear_layers_hidden_dim), name='linear_layer_graph_placeholder_final')\n",
    "            w1_final = tf.Variable(tf.random.normal(shape=(self.linear_layers_hidden_dim, self.emb_dim), dtype=tf.float32), name='w1_final')\n",
    "            b1_final = tf.Variable(tf.constant(0.1, shape=(1, self.emb_dim), dtype=tf.float32), name='b1_final')\n",
    "            y1_final = tf.add(tf.matmul(inputs_final, w1_final), b1_final)\n",
    "            \n",
    "            #here multhead attention finish\n",
    "            \n",
    "            # now onto add and norm layer\n",
    "            inputs_for_norm = tf.compat.v1.placeholder(dtype=tf.float32, shape=(self.seq_len, self.emb_dim), name='InputPlaceholderNorm')\n",
    "            add_op = tf.add(inputs_for_norm,y1_final)\n",
    "            meaned_output = tf.reduce_mean(y1_final, axis=1)\n",
    "            std_output = tf.math.reduce_std(y1_final, axis=0)\n",
    "            x0 = tf.transpose(tf.subtract(tf.transpose(add_op), meaned_output))\n",
    "            divisor = tf.sqrt(tf.add(std_output, tf.constant(0.001, dtype=tf.float32)))\n",
    "            res = tf.divide(x0, divisor)\n",
    "            # finsish add and norm layer\n",
    "            \n",
    "            #feed forward network\n",
    "            inputs_for_ffn = tf.compat.v1.placeholder(dtype=tf.float32, shape=(self.seq_len, self.emb_dim), name='InputPlaceholderLinear')\n",
    "            w1_ffn = tf.Variable(tf.random.normal(shape=(self.emb_dim, self.ffn_hiiden_dim), dtype=tf.float32), name='w1_ffn')\n",
    "            b1_ffn = tf.Variable(tf.constant(0.1, shape=(1, self.ffn_hiiden_dim), dtype=tf.float32), name='b1_ffn')\n",
    "            y1_ffn = tf.nn.relu(tf.add(tf.matmul(inputs_for_ffn, w1_ffn), b1_ffn))\n",
    "            w2_ffn = tf.Variable(tf.random.normal(shape=(self.ffn_hiiden_dim, self.emb_dim), dtype=tf.float32), name='w2_ffn')\n",
    "            b2_ffn = tf.Variable(tf.constant(0.1, shape=(1, self.emb_dim), dtype=tf.float32), name='b2_ffn')\n",
    "            y2_ffn = tf.add(tf.matmul(y1_ffn, w2_ffn), b2_ffn)\n",
    "            #end of feed forward neural network\n",
    "            \n",
    "            #now onto final add and norm layer\n",
    "            inputs_for_norm_final = tf.compat.v1.placeholder(dtype=tf.float32, shape=(self.seq_len, self.emb_dim), name='InputPlaceholderLinear')\n",
    "            add_op_final = tf.add(inputs_for_norm_final,y2_ffn)\n",
    "            meaned_output_final = tf.reduce_mean(y2_ffn, axis=1)\n",
    "            std_output_final = tf.math.reduce_std(y2_ffn, axis=0)\n",
    "            x0_final = tf.transpose(tf.subtract(tf.transpose(add_op_final), meaned_output_final))\n",
    "            divisor_final = tf.sqrt(tf.add(std_output_final, tf.constant(0.001, dtype=tf.float32)))\n",
    "            res_final = tf.divide(x0_final, divisor_final)\n",
    "            #driver code\n",
    "            \n",
    "            init_op = tf.compat.v1.global_variables_initializer()\n",
    "            attention_head.run(init_op)\n",
    "            \n",
    "            sentences = []\n",
    "            final_output = []\n",
    "            \n",
    "            for sent in X:\n",
    "                one_hot_encoded = tf.one_hot(sent, dtype=tf.float32, depth=self.vocab_size)\n",
    "                sent_vector = []\n",
    "                for pos,word in enumerate(tf.unstack(one_hot_encoded)):\n",
    "                    word = tf.reshape(word, shape=(1,len(vocab))).eval(session=attention_head)\n",
    "                    output_1 = positional_out.eval(session=attention_head, feed_dict={inputs:word, input_pos:pos})\n",
    "                    sent_vector.append(output_1)\n",
    "                sent_vector = np.asarray(sent_vector).reshape((self.seq_len,self.emb_dim))\n",
    "                output_2 = final_output_of_one_head.eval(session=attention_head, feed_dict={inputs_k:sent_vector, inputs_q:sent_vector, inputs_v:sent_vector})\n",
    "                output_3 = res.eval(session=attention_head, feed_dict={inputs_final:output_2, inputs_for_norm:sent_vector})\n",
    "                output_4 = res_final.eval(session=attention_head, feed_dict={inputs_for_ffn:output_3, inputs_for_norm_final:output_3})\n",
    "                sentences.append(output_4)\n",
    "            final_output.append(sentences)\n",
    "            return np.asarray(final_output).reshape(len(final_output[0]), self.seq_len, self.emb_dim)\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8c1441a7-ba0e-4bdc-b86a-3fbc55462039",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.compat.v1.Session()\n",
    "init_op = tf.compat.v1.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "obj = TransformerEncoder(len(vocab), 5, 128, 512, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ba7cce37-66d2-4cc2-9c9e-8b71d2abd0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X =[[0, 1, 2, 3, 4],\n",
    "[5, 1, 2, 3, 6]]\n",
    "output = obj.call(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a160ca34-eda8-4bfd-823c-6ef1a67d12f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 177.64572  , -164.9207   ,   41.02332  , ...,   28.80564  ,\n",
       "         -104.76409  ,   53.713436 ],\n",
       "        [ 177.41264  , -166.20238  ,   41.256176 , ...,   28.025219 ,\n",
       "         -103.48805  ,   54.316177 ],\n",
       "        [ 179.73714  , -109.02429  ,  -80.95682  , ...,  -80.20761  ,\n",
       "         -108.17632  ,    1.9724544],\n",
       "        [ 177.80676  , -165.60052  ,   39.81535  , ...,   27.40807  ,\n",
       "         -103.270294 ,   53.764374 ],\n",
       "        [   8.146667 , -205.72935  ,   34.00499  , ...,    9.163757 ,\n",
       "           10.055157 ,  -68.68278  ]],\n",
       "\n",
       "       [[  69.70959  ,  -38.025944 ,  518.7505   , ..., -113.50997  ,\n",
       "          -32.82434  ,  -96.92849  ],\n",
       "        [ 180.44513  , -116.1046   ,  564.216    , ...,  -52.037415 ,\n",
       "          -99.88502  ,   52.78704  ],\n",
       "        [ 179.19827  , -116.02184  ,  567.8015   , ...,  -51.307636 ,\n",
       "         -101.023834 ,   51.634747 ],\n",
       "        [ 180.43875  , -116.37273  ,  557.4599   , ...,  -52.848827 ,\n",
       "         -100.2059   ,   52.353092 ],\n",
       "        [  37.222523 ,  -92.36375  ,  505.6709   , ..., -117.73744  ,\n",
       "           20.195349 ,    2.2981882]]], dtype=float32)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da019ad0-0736-4999-9bb2-550af53f57ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f7627c97-88b2-4388-b161-ec9c8dc1ce02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_29:0' shape=(3, 2) dtype=int32> <tf.Variable 'Variable_30:0' shape=(3,) dtype=int32>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 2],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session()\n",
    "t1 = (tf.Variable([[1,2],[3,4],[5,6]]))\n",
    "t2 = tf.Variable([1,2,3])\n",
    "print(t1, t2)\n",
    "t3 = tf.subtract(tf.transpose(t1),t2)\n",
    "new = tf.transpose(t3)\n",
    "init_op = tf.compat.v1.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "\n",
    "new.eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5c301a8b-0bad-45cb-ae84-ac9a2a2a33b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('0', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2499e418-b60b-4d0c-99d8-05ad8ed62580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_10:0' shape=(63878, 20) dtype=int32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74768273-38dd-4ef6-8335-77118540c5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_10:0' shape=(63878, 20) dtype=int32>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7cc0cb1f-0ef2-45fb-a36e-16ba636b62e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bert_vocab_uncased.txt', 'r') as f:\n",
    "    vocab_ = f.read()\n",
    "    vocab = {}\n",
    "    for i in (vocab_.split()):\n",
    "        if i not in vocab.keys():\n",
    "            vocab[i] = len(vocab)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "57303727-cafa-457f-9f65-e242d29c27e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('D:\\Transformers Implementation\\Language Model\\Data\\enwiki20201020\\\\00c2bfc7-57db-496e-9d5c-d62f8d8119e3.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "811d27e1-7eee-450e-9b3c-e8f98a3abcd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"M-137 was a state trunkline highway in the US state of Michigan that served as a spur route to the Interlochen Center for the Arts and Interlochen State Park. It started south of the park and ran north between two lakes in the area and through the community of Interlochen to US Highway 31 (US 31) in Grand Traverse County. The highway was first shown without a number label on maps in 1930 and labeled after an extension the next year. The highway's current routing was established in the 1950s. Jurisdiction of the roadway was transferred from the Michigan Department of Transportation (MDOT) to the Grand Traverse County Road Commission in June 2020, and the highway designation was decommissioned in the process; signage was removed by August 2020 to reflect the changeover. ==Route description== M-137 began at the southern end of Interlochen State Park at an intersection with Vagabond Lane. Farther south, the roadway continues toward Green Lake Airport as County Road 137 (CR 137), also known as Karlin Road. The state highway was a two-lane road that meandered north, passing the entrance to the state park and near the Interlochen Center for the Arts. The road continued along the isthmus between Green and Duck lakes. North of the school, the highway passed through a wooded section before entering the community of Interlochen itself near the Green Lake Township Hall. There M-137 ran almost due north before terminating at its connection with the rest of the state trunkline system at US 31 at Interlochen Corners. The roadway continues north of US 31 as South Long Lake Road after the M-137 designation ended. M-137 was maintained by MDOT like other state highways in Michigan. According to the department in 2010, 4,868 vehicles used the highway daily on average. No section of M-137 had been listed on the National Highway System, a network of roads important to the country's economy, defense, and mobility. ==History== reassurance marker near Diamond Park Road and the entrance to Interlochen Center for the Arts, May 2018 A highway along the route of M-137 connecting US 31 south to the state park was added to the state highway system during the first half of 1930, initially lacking a designation label on the state maps of the time. This routing was extended by and labelled as M-137 on maps in 1931. The former route through the campus of the Interlochen Center for the Arts was abandoned as a roadway on March 26, 1956, after M-137 was realigned to pass to the east of the school and extended further south through the state park area. On April 30, 2020, the GRCTC was to vote on a resolution to accept jurisdiction over M-137 from MDOT, effective June 1, 2020; the board approved the resolution. MDOT announced on August 6, 2020, that jurisdiction had been transferred at the beginning of June and that all M-137 signage had since been removed. ==Major intersections== ==See also== * ==References== ==External links== *Former M-137 at Michigan Highways 137 Category:Transportation in Grand Traverse County, Michigan \""
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1d0d7001-facb-4ec9-85aa-d050c9618502",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sent = data[0]['text'].split('.')\n",
    "new_data = []\n",
    "for i in list_of_sent:\n",
    "    t = []\n",
    "    for k in i.split():\n",
    "        if k in vocab.keys():\n",
    "            t.append(vocab[k])\n",
    "    new_data.append(t)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a1593670-1615-4c0b-9a34-6c9f4f05d09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2001,\n",
       "  1037,\n",
       "  2110,\n",
       "  3307,\n",
       "  1999,\n",
       "  1996,\n",
       "  2110,\n",
       "  1997,\n",
       "  2008,\n",
       "  2366,\n",
       "  2004,\n",
       "  1037,\n",
       "  12996,\n",
       "  2799,\n",
       "  2000,\n",
       "  1996,\n",
       "  2005,\n",
       "  1996,\n",
       "  1998]]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "27f3179a-a362-41a8-8270-9b79c411c1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "mask_positions = []\n",
    "target_values = []\n",
    "weights = []\n",
    "new_sequences = []\n",
    "done_data = []\n",
    "for m, i in (enumerate(new_data)):\n",
    "    if len(i) > 5:\n",
    "        t = []\n",
    "        t_1 = []\n",
    "        t_2 = []\n",
    "        for k, j in enumerate(i):\n",
    "            if k < 20:\n",
    "                if random.random() <= 0.15:\n",
    "                    if len(t) < 3:\n",
    "                        t.append(k)\n",
    "                        t_1.append(j)\n",
    "                        t_2.append(1)\n",
    "                        i[k] = vocab['[MASK]']\n",
    "        weights.append(t_2 if len(t_2) == 3 else (\n",
    "            t_2 + [0]*(3-len(t_2))))\n",
    "        mask_positions.append(t if len(t) == 3 else (\n",
    "            t+[0]*(3-len(t_2))))\n",
    "        target_values.append(t_1 if len(t_1) == 3 else (\n",
    "            t_1+[0]*(3-len(t_1))))\n",
    "        new_sequences.append(\n",
    "            i[:20]+[0]*(20-len(i[:20])))\n",
    "\n",
    "done_data.append(({'tokens': new_sequences, 'mask_positions': mask_positions},\n",
    "                                          target_values, weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "897988cc-e928-458f-8888-0ff7fb42a1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2001,\n",
       "  1037,\n",
       "  2110,\n",
       "  3307,\n",
       "  1999,\n",
       "  1996,\n",
       "  2110,\n",
       "  103,\n",
       "  2008,\n",
       "  2366,\n",
       "  2004,\n",
       "  1037,\n",
       "  12996,\n",
       "  103,\n",
       "  2000,\n",
       "  1996,\n",
       "  2005,\n",
       "  1996,\n",
       "  1998,\n",
       "  0],\n",
       " [7, 13, 0],\n",
       " [1997, 2799, 0],\n",
       " [1, 1, 0])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sequences[0],mask_positions[0], target_values[0], weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0c7e7890-3d9d-49b0-89d8-9a70843eb9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(done_data[0][0]['tokens']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "164f5245-48a9-4029-9de9-f4ddde21906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = data[0]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3ead19ae-94bb-4072-b742-ba0c12713f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_nlp\n",
    "te = TransformerEncoder(\n",
    "        vocab_size=len(vocab),\n",
    "        seq_len=20,\n",
    "        emb_dim=128,\n",
    "        linear_layers_hidden_dim = 128,\n",
    "        ffn_hiiden_dim = 128)\n",
    "encoded_tokens = te.call(done_data[0][0]['tokens'])\n",
    "outputs = keras_nlp.layers.MLMHead(\n",
    "             activation=\"softmax\",vocabulary_size = len(vocab))\n",
    "\n",
    "final_out = outputs(encoded_tokens, mask_positions=done_data[0][0][\"mask_positions\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "59f6e2d8-f0ee-4eea-9c1a-9350e2b70f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20, 128)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "5f735e12-0424-43f3-b87f-9ab394cbe68f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [171]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m init_op \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mglobal_variables_initializer()\n\u001b[1;32m----> 2\u001b[0m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_op\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m final_out\u001b[38;5;241m.\u001b[39meval(session\u001b[38;5;241m=\u001b[39msess)\n",
      "File \u001b[1;32mD:\\Transformers Implementation\\English to Hindi\\transformers\\lib\\site-packages\\tensorflow\\python\\client\\session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m    971\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32mD:\\Transformers Implementation\\English to Hindi\\transformers\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1116\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;66;03m# Check session.\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed:\n\u001b[1;32m-> 1116\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempted to use a closed Session.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mversion \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1118\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe Session graph is empty. Add operations to the \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1119\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraph before calling run().\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": [
    "init_op = tf.compat.v1.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "final_out.eval(session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "50ef14e6-075f-4d7f-8544-d1917fda06ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 3, 30522])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "80d0fd2e-ea31-4dc2-becf-6c9cafb3b4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['[MASK]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b83fa570-250e-4971-987a-25ef6623d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.asarray(done_data[0][1]).reshape((1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7a5bdd33-285f-4f87-bcfc-7f3140d70a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2110, 1996,    0])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2859b028-8845-4112-8f8e-d92a4f81f88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_output = tf.one_hot(done_data[0][1], depth=len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c3a51196-1c4e-45f2-ad4e-cc82a71b56d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'one_hot_66:0' shape=(20, 3, 30522) dtype=float32>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9ec5207c-c16a-4a6b-b772-8bd81c9c0d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.abs(tf.subtract(final_out,one_hot_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "948b7416-418c-4a75-9727-568a8097176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.compat.v1.train.GradientDescentOptimizer(1.0).minimize(loss, var_list=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "daa0d344-724c-4fb5-84d7-03cc764553df",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_vocab = {v:k for k,v in vocab.items()}\n",
    "correctly_classified = 0\n",
    "for epoch in range(10):\n",
    "    _,loss_val = sess.run([train,loss])\n",
    "   \n",
    "    # for k in loss_val[0]:\n",
    "    #     print(tf.add_n(k).eval(session=sess))\n",
    "    # print(\"\\n\\n\")\n",
    "        \n",
    "def predict():\n",
    "    enc_seq = te.call(done_data[0][0]['tokens'])\n",
    "    f_out = outputs(enc_seq, mask_positions=done_data[0][0][\"mask_positions\"])\n",
    "    for i in f_out.eval(session=sess)[0]:\n",
    "        print(np.argmax(i))\n",
    "# predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ea4a7313-a0a9-4cc3-ba3c-04e88eba0ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "820784eb-d2b6-47f3-a888-dd4e0c68bebc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e349b68c-3fad-4895-bad8-ef07ea2ad964",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\users\\naman\\appdata\\local\\programs\\python\\python38\\lib\\contextlib.py:131\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 131\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Transformers Implementation\\English to Hindi\\transformers\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:5868\u001b[0m, in \u001b[0;36m_DefaultGraphStack.get_controller\u001b[1;34m(self, default)\u001b[0m\n\u001b[0;32m   5866\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(_DefaultGraphStack,\n\u001b[0;32m   5867\u001b[0m              \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mget_controller(default) \u001b[38;5;28;01mas\u001b[39;00m g, context\u001b[38;5;241m.\u001b[39mgraph_mode():\n\u001b[1;32m-> 5868\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m g\n\u001b[0;32m   5869\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   5870\u001b[0m   \u001b[38;5;66;03m# If an exception is raised here it may be hiding a related exception in\u001b[39;00m\n\u001b[0;32m   5871\u001b[0m   \u001b[38;5;66;03m# the try-block (just above).\u001b[39;00m\n",
      "Input \u001b[1;32mIn [53]\u001b[0m, in \u001b[0;36mTransformerEncoder.call\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pos,word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tf\u001b[38;5;241m.\u001b[39munstack(one_hot_encoded)):\n\u001b[1;32m--> 125\u001b[0m     word \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_head\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     output_1 \u001b[38;5;241m=\u001b[39m positional_out\u001b[38;5;241m.\u001b[39meval(session\u001b[38;5;241m=\u001b[39mattention_head, feed_dict\u001b[38;5;241m=\u001b[39m{inputs:word, input_pos:pos})\n",
      "File \u001b[1;32mD:\\Transformers Implementation\\English to Hindi\\transformers\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:993\u001b[0m, in \u001b[0;36mTensor.eval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;124;03m\"\"\"Evaluates this tensor in a `Session`.\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \n\u001b[0;32m    972\u001b[0m \u001b[38;5;124;03mNote: If you are not using `compat.v1` libraries, you should not need this,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;124;03m  A numpy array corresponding to the value of this tensor.\u001b[39;00m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 993\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_eval_using_default_session\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Transformers Implementation\\English to Hindi\\transformers\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:5792\u001b[0m, in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   5789\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use the given session to evaluate tensor: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5790\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe tensor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms graph is different from the session\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5791\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 5792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Transformers Implementation\\English to Hindi\\transformers\\lib\\site-packages\\tensorflow\\python\\client\\session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n",
      "File \u001b[1;32mD:\\Transformers Implementation\\English to Hindi\\transformers\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1191\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[1;32m-> 1191\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Transformers Implementation\\English to Hindi\\transformers\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1371\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1371\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Transformers Implementation\\English to Hindi\\transformers\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1378\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mD:\\Transformers Implementation\\English to Hindi\\transformers\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1360\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[0;32m   1359\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[1;32m-> 1360\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extend_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1361\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m   1362\u001b[0m                                   target_list, run_metadata)\n",
      "File \u001b[1;32mD:\\Transformers Implementation\\English to Hindi\\transformers\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1401\u001b[0m, in \u001b[0;36mBaseSession._extend_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1400\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39m_session_run_lock():  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 1401\u001b[0m   \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExtendSession\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [152]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m      2\u001b[0m   \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     enc_seq \u001b[38;5;241m=\u001b[39m \u001b[43mte\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     f_out \u001b[38;5;241m=\u001b[39m outputs(enc_seq, mask_positions\u001b[38;5;241m=\u001b[39mdone_data[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask_positions\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      5\u001b[0m     loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mabs(tf\u001b[38;5;241m.\u001b[39msubtract(final_out,one_hot_output))\n",
      "Input \u001b[1;32mIn [53]\u001b[0m, in \u001b[0;36mTransformerEncoder.call\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    132\u001b[0m     sentences\u001b[38;5;241m.\u001b[39mappend(output_4)\n\u001b[0;32m    133\u001b[0m final_output\u001b[38;5;241m.\u001b[39mappend(sentences)\n\u001b[1;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(final_output)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(final_output[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb_dim)\n",
      "File \u001b[1;32mD:\\Transformers Implementation\\English to Hindi\\transformers\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1653\u001b[0m, in \u001b[0;36mSession.__exit__\u001b[1;34m(self, exec_type, exec_value, exec_tb)\u001b[0m\n\u001b[0;32m   1651\u001b[0m close_thread\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1652\u001b[0m close_thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m-> 1653\u001b[0m \u001b[43mclose_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m30.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m close_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m   1655\u001b[0m   logging\u001b[38;5;241m.\u001b[39merror(\n\u001b[0;32m   1656\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSession failed to close after 30 seconds. Continuing after this \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1657\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoint may leave your program in an undefined state.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\naman\\appdata\\local\\programs\\python\\python38\\lib\\threading.py:1015\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock()\n\u001b[0;32m   1012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1013\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m-> 1015\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\naman\\appdata\\local\\programs\\python\\python38\\lib\\threading.py:1027\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# already determined that the C code is done\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_stopped\n\u001b[1;32m-> 1027\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1028\u001b[0m     lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m   1029\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "  # Forward pass\n",
    "    enc_seq = te.call(done_data[0][0]['tokens'])\n",
    "    f_out = outputs(enc_seq, mask_positions=done_data[0][0][\"mask_positions\"])\n",
    "    loss = tf.abs(tf.subtract(final_out,one_hot_output))\n",
    "    grad = tape.gradient(loss, tf.compat.v1.trainable_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052ac435-3cbb-41c6-8c9c-944f62520623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers Kernel",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6de98d15-ec49-4fd8-a8bc-f81f9180764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70d31f75-29c0-47dc-bd9a-241c6c723cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be42918a-1043-4a47-8924-29dafda36951",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c2f6016-f0bb-4ad0-aedc-36dc449460b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder:\n",
    "    \n",
    "    def __new__(self, vocab_size, seq_len, emb_dim, linear_layers_hidden_dim, ffn_hiiden_dim):\n",
    "\n",
    "        # Positional Embeddings calculation\n",
    "        linear_layers_hidden_dim = emb_dim\n",
    "        def sin_positions(pos):\n",
    "            t = tf.multiply(tf.constant(2, dtype=tf.float32), tf.range(0, emb_dim, dtype=tf.float32))\n",
    "            t1 = tf.pow(tf.constant(10000, dtype=tf.float32), tf.divide(t, emb_dim))\n",
    "            position_tensor = tf.fill([1, emb_dim], pos)\n",
    "            sin_fun_input = tf.divide(position_tensor, t1)\n",
    "            sin_output = tf.sin(sin_fun_input)\n",
    "            return sin_output\n",
    "\n",
    "        def cos_positions(pos):\n",
    "            t = tf.multiply(tf.constant(2, dtype=tf.float32), tf.range(0, emb_dim, dtype=tf.float32))\n",
    "            t1 = tf.pow(tf.constant(10000, dtype=tf.float32), tf.divide(t, emb_dim))\n",
    "            position_tensor = tf.fill([1, emb_dim], pos)\n",
    "            sin_fun_input = tf.divide(position_tensor, t1)\n",
    "            cos_output = tf.cos(sin_fun_input)\n",
    "            return cos_output\n",
    "        \n",
    "        positonal_embeddings = tf.reshape(\n",
    "            tf.map_fn(\n",
    "                lambda pos: tf.cond(\n",
    "                    tf.equal(\n",
    "                        tf.experimental.numpy.mod(pos,2),0),\n",
    "                    lambda : sin_positions(pos),\n",
    "                    lambda : cos_positions(pos)),\n",
    "                tf.range(seq_len, dtype=tf.float32)),\n",
    "            shape=(seq_len,emb_dim))\n",
    "        \n",
    "        # start of embedding graph\n",
    "        # input_data : signature = [1,2]\n",
    "        input_data = tf.compat.v1.placeholder(dtype=tf.int32, shape=(1, seq_len), name='firstInputPlaceholder')\n",
    "        one_hot_encoded = tf.one_hot(input_data, dtype=tf.float32, depth=vocab_size)\n",
    "        \n",
    "\n",
    "        @tf.function\n",
    "        def get_embeddings(input_word):\n",
    "            w1 = tf.Variable(tf.random.normal(shape=(vocab_size, emb_dim)), dtype=tf.float32, name='w1')\n",
    "            b1 = tf.Variable(tf.constant(0.2, shape=(1, emb_dim), dtype=tf.float32), name='b1')\n",
    "            y1 = tf.add(tf.matmul(input_word, w1), b1)\n",
    "            return y1\n",
    "        \n",
    "        for word in tf.unstack(one_hot_encoded):\n",
    "            # print(word.eval(session=sess, feed_dict={input_data:np.asarray([1,2,3,4,5]).reshape((1,5))}))\n",
    "            embeddings = get_embeddings(one_hot_encoded)\n",
    "        \n",
    "        positional_out = tf.add(positonal_embeddings, embeddings)\n",
    "        # init_op = tf.compat.v1.global_variables_initializer()\n",
    "        # sess.run(init_op)\n",
    "        # # end of embedding graph\n",
    "        # return positonal_embeddings,embeddings, input_data\n",
    "        #value linear layer\n",
    "        @tf.function\n",
    "        def multihead():\n",
    "            w1_v = tf.Variable(tf.random.normal(shape=(emb_dim, linear_layers_hidden_dim), dtype=tf.float32), name='w1_v')\n",
    "            b1_v = tf.Variable(tf.constant(0.1, shape=(1, linear_layers_hidden_dim), dtype=tf.float32), name='b1_v')\n",
    "            y1_v = tf.squeeze(tf.add(tf.matmul(positional_out, w1_v), b1_v))\n",
    "\n",
    "            #key linear layer\n",
    "\n",
    "            w1_k = tf.Variable(tf.random.normal(shape=(emb_dim, linear_layers_hidden_dim), dtype=tf.float32), name='w1_k')\n",
    "            b1_k = tf.Variable(tf.constant(0.1, shape=(1, linear_layers_hidden_dim), dtype=tf.float32), name='b1_k')\n",
    "            y1_k = tf.squeeze(tf.add(tf.matmul(positional_out, w1_k), b1_k))\n",
    "            #query linear layer\n",
    "            \n",
    "            w1_q = tf.Variable(tf.random.normal(shape=(emb_dim, linear_layers_hidden_dim), dtype=tf.float32), name='w1_q')\n",
    "            b1_q = tf.Variable(tf.constant(0.1, shape=(1, linear_layers_hidden_dim), dtype=tf.float32), name='b1_q')\n",
    "            y1_q = tf.squeeze(tf.add(tf.matmul(positional_out, w1_q), b1_q))\n",
    "            \n",
    "            transposed_key_mat = tf.transpose(y1_k, name='key_transpose_op')\n",
    "          \n",
    "            attention_filter = tf.divide(tf.matmul(y1_q, transposed_key_mat), tf.sqrt(tf.constant(emb_dim, dtype=tf.float32)), name='attention_filter')\n",
    "            softmaxed_attention_filter = tf.nn.softmax(attention_filter, name='softmax_of_attention_filter')\n",
    "            final_output_of_one_head = tf.matmul(softmaxed_attention_filter, y1_v, name='final_matmul')\n",
    "   \n",
    "            # final linear layer\n",
    "      \n",
    "            w1_final = tf.Variable(tf.random.normal(shape=(linear_layers_hidden_dim, emb_dim), dtype=tf.float32), name='w1_final')\n",
    "            b1_final = tf.Variable(tf.constant(0.1, shape=(1, emb_dim), dtype=tf.float32), name='b1_final')\n",
    "            y1_final = tf.add(tf.matmul(final_output_of_one_head, w1_final), b1_final)\n",
    "\n",
    "            # now onto add and norm layer\n",
    "            \n",
    "            add_op = tf.squeeze(tf.add(positional_out,final_output_of_one_head))\n",
    "            meaned_output = tf.reduce_mean(y1_final, axis=1)\n",
    "            std_output = tf.math.reduce_std(y1_final, axis=0)\n",
    "            x0 = tf.transpose(tf.subtract(tf.transpose(add_op), meaned_output))\n",
    "            divisor = tf.sqrt(tf.add(std_output, tf.constant(0.001, dtype=tf.float32)))\n",
    "            res = tf.divide(x0, divisor)\n",
    "            # finsish add and norm layer\n",
    "\n",
    "            #feed forward network\n",
    "           \n",
    "            w1_ffn = tf.Variable(tf.random.normal(shape=(emb_dim, ffn_hiiden_dim), dtype=tf.float32), name='w1_ffn')\n",
    "            b1_ffn = tf.Variable(tf.constant(0.1, shape=(1, ffn_hiiden_dim), dtype=tf.float32), name='b1_ffn')\n",
    "            y1_ffn = tf.nn.relu(tf.add(tf.matmul(res, w1_ffn), b1_ffn))\n",
    "            w2_ffn = tf.Variable(tf.random.normal(shape=(ffn_hiiden_dim, emb_dim), dtype=tf.float32), name='w2_ffn')\n",
    "            b2_ffn = tf.Variable(tf.constant(0.1, shape=(1, emb_dim), dtype=tf.float32), name='b2_ffn')\n",
    "            y2_ffn = tf.add(tf.matmul(y1_ffn, w2_ffn), b2_ffn)\n",
    "            #end of feed forward neural network\n",
    "            \n",
    "            #now onto final add and norm layer\n",
    "            add_op_final = tf.add(res,y2_ffn)\n",
    "            \n",
    "            meaned_output_final = tf.reduce_mean(y2_ffn, axis=1)\n",
    "\n",
    "            std_output_final = tf.math.reduce_std(y2_ffn, axis=0)\n",
    "            x0_final = tf.transpose(tf.subtract(tf.transpose(add_op_final), meaned_output_final))\n",
    "            divisor_final = tf.sqrt(tf.add(std_output_final, tf.constant(0.001, dtype=tf.float32)))\n",
    "            res_final = tf.divide(x0_final, divisor_final)\n",
    "        \n",
    "            return res_final\n",
    "\n",
    "        init_op = tf.compat.v1.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        return multihead(), input_data\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd53edc4-d113-4c40-a341-e9eef996e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bert_vocab_uncased.txt', 'r') as f:\n",
    "    vocab_ = f.read()\n",
    "    vocab = {}\n",
    "    for i in (vocab_.split()):\n",
    "        if i not in vocab.keys():\n",
    "            vocab[i] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0679f8c3-cf6b-457b-b541-8510de7c7faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:\\Transformers Implementation\\Language Model\\Data\\enwiki20201020\\\\00c2bfc7-57db-496e-9d5c-d62f8d8119e3.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7ad7b566-e7ee-47c4-96bd-fe86ba5297de",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sent = data[0]['text'].split('.')\n",
    "new_data = []\n",
    "for i in list_of_sent:\n",
    "    t = []\n",
    "    for k in i.split():\n",
    "        if k in vocab.keys():\n",
    "            t.append(vocab[k])\n",
    "    new_data.append(t)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "63551fab-34f7-4f6a-ba5e-d33e1aa8222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "mask_positions = []\n",
    "target_values = []\n",
    "weights = []\n",
    "new_sequences = []\n",
    "done_data = []\n",
    "for m, i in (enumerate(new_data)):\n",
    "    if len(i) > 5:\n",
    "        t = []\n",
    "        t_1 = []\n",
    "        t_2 = []\n",
    "        for k, j in enumerate(i):\n",
    "            if k < 20:\n",
    "                if random.random() <= 0.15:\n",
    "                    if len(t) < 3:\n",
    "                        t.append(k)\n",
    "                        t_1.append(j)\n",
    "                        t_2.append(1)\n",
    "                        i[k] = vocab['[MASK]']\n",
    "        weights.append(t_2 if len(t_2) == 3 else (\n",
    "            t_2 + [0]*(3-len(t_2))))\n",
    "        mask_positions.append(t if len(t) == 3 else (\n",
    "            t+[0]*(3-len(t_2))))\n",
    "        target_values.append(t_1 if len(t_1) == 3 else (\n",
    "            t_1+[0]*(3-len(t_1))))\n",
    "        new_sequences.append(\n",
    "            i[:20]+[0]*(20-len(i[:20])))\n",
    "\n",
    "done_data.append(({'tokens': new_sequences, 'mask_positions': mask_positions},\n",
    "                                          target_values, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9d61a4d7-ab80-4928-b11e-dc06b801c772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'tokens': [[2001,\n",
       "     1037,\n",
       "     2110,\n",
       "     3307,\n",
       "     1999,\n",
       "     1996,\n",
       "     2110,\n",
       "     1997,\n",
       "     2008,\n",
       "     2366,\n",
       "     2004,\n",
       "     1037,\n",
       "     12996,\n",
       "     2799,\n",
       "     2000,\n",
       "     1996,\n",
       "     2005,\n",
       "     103,\n",
       "     1998,\n",
       "     0]],\n",
       "   'mask_positions': [[17, 0, 0]]},\n",
       "  [[1996, 0, 0]],\n",
       "  [[1, 0, 0]])]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503a9cab-cf4d-4f69-8116-5f8cc6ac3b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a784662f-f6a0-4edd-bc5f-46e61654fc1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "resource: Attempting to capture an EagerTensor without building a function.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m te \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mTransformerEncoder.__new__\u001b[1;34m(self, vocab_size, seq_len, emb_dim, linear_layers_hidden_dim, ffn_hiiden_dim)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y1\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39munstack(one_hot_encoded):\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m# print(word.eval(session=sess, feed_dict={input_data:np.asarray([1,2,3,4,5]).reshape((1,5))}))\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mone_hot_encoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m positional_out \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39madd(positonal_embeddings, embeddings)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# init_op = tf.compat.v1.global_variables_initializer()\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# sess.run(init_op)\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# # end of embedding graph\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# return positonal_embeddings,embeddings, input_data\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m#value linear layer\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Transformers Implementation\\English to Hindi\\transformers\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mD:\\Transformers Implementation\\English to Hindi\\transformers\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1588\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1586\u001b[0m     graph \u001b[38;5;241m=\u001b[39m get_default_graph()\n\u001b[0;32m   1587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mbuilding_function:\n\u001b[1;32m-> 1588\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1589\u001b[0m           _add_error_prefix(\n\u001b[0;32m   1590\u001b[0m               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to capture an EagerTensor without \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1591\u001b[0m               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilding a function.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1592\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m   1593\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mcapture(value, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m   1595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: resource: Attempting to capture an EagerTensor without building a function."
     ]
    }
   ],
   "source": [
    "te = TransformerEncoder(7,10,128,128,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a31f577-9ce1-49fd-82f3-e2346d0e57c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(\"output\", sess.graph)\n",
    "    print(sess.run(h))\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers Kernel",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
